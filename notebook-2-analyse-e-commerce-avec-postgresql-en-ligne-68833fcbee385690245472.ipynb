{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I43NxonzOSDg"
      },
      "source": [
        "# Notebook 2 - SQL avec vraies bases de donn√©es\n",
        "## Analyse e-commerce avec PostgreSQL en ligne\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JItQV6o4Ojrm"
      },
      "source": [
        "\n",
        "### üéØ Objectifs p√©dagogiques\n",
        "- Connecter Python √† une vraie base de donn√©es PostgreSQL\n",
        "- √âcrire des requ√™tes SQL complexes sur des donn√©es r√©elles\n",
        "- Impl√©menter des analyses RFM avec SQL\n",
        "- Int√©grer SQL et pandas pour des analyses avanc√©es\n",
        "- G√©rer les connexions et la s√©curit√©\n",
        "\n",
        "### üõçÔ∏è Contexte du projet\n",
        "Vous analysez les donn√©es d'un vrai dataset e-commerce (Brazilian E-Commerce Public Dataset) h√©berg√© sur une base PostgreSQL.\n",
        "\n",
        "Objectif : cr√©er une segmentation client√®le pour optimiser les campagnes marketing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K79TBMVvOuoj"
      },
      "source": [
        "## Partie 1 : Connexion √† la base de donn√©es r√©elle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq7n18iwPBPe"
      },
      "source": [
        "### üîß Installation et configuration\n",
        "\n",
        "\n",
        "# Installation des d√©pendances\n",
        "\n",
        "\n",
        "```\n",
        "pip install psycopg2-binary sqlalchemy pandas python-dotenv\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sqlalchemy in ./.venv/lib/python3.12/site-packages (2.0.42)\n",
            "Requirement already satisfied: psycopg2-binary in ./.venv/lib/python3.12/site-packages (2.9.10)\n",
            "Requirement already satisfied: python-dotenv in ./.venv/lib/python3.12/site-packages (1.1.1)\n",
            "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.3.1)\n",
            "Requirement already satisfied: greenlet>=1 in ./.venv/lib/python3.12/site-packages (from sqlalchemy) (3.2.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in ./.venv/lib/python3.12/site-packages (from sqlalchemy) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install sqlalchemy psycopg2-binary python-dotenv pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_NuY2FHuOhu3"
      },
      "outputs": [],
      "source": [
        "import psycopg2 # Permet de se connecter √† une base de donn√©es PostgreSQL directement en Python\n",
        "import pandas as pd # Biblioth√®que pour manipuler des donn√©es sous forme de tableaux (DataFrame)\n",
        "import numpy as np  # Fournit des outils pour les calculs num√©riques (matrices, statistiques, etc.)\n",
        "from sqlalchemy import create_engine # Cr√©e un \"moteur\" pour se connecter aux bases de donn√©es (PostgreSQL, MySQL, etc.)\n",
        "import matplotlib.pyplot as plt  # faire des graphiques et des visualisations\n",
        "import seaborn as sns # Biblioth√®que bas√©e sur matplotlib pour faire de beaux graphiques statistiques\n",
        "from datetime import datetime, timedelta # Permet de g√©rer et manipuler des dates et dur√©es ajouter des jours...\n",
        "import os   # Sert √† interagir avec le syst√®me d'exploitation exp : lire des variables d'environnement\n",
        "from dotenv import load_dotenv # Permet de charger les variables stock√©es dans un fichier .env : MDP, cl√©s API..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEbORVz5PXMa"
      },
      "source": [
        "### üåê Base de donn√©es PostgreSQL gratuite (ElephantSQL)\n",
        "\n",
        "**Option 1 : ElephantSQL (20MB gratuit)**\n",
        "1. Cr√©ez un compte sur [elephantsql.com](https://www.elephantsql.com/)\n",
        "2. Cr√©ez une instance \"Tiny Turtle\" (gratuite)\n",
        "3. R√©cup√©rez vos credentials\n",
        "\n",
        "**Option 2 : Supabase (500MB gratuit)**\n",
        "1. Cr√©ez un compte sur [supabase.com](https://supabase.com/)\n",
        "2. Cr√©ez un nouveau projet\n",
        "3. R√©cup√©rez l'URL de connexion PostgreSQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Charger les variables d'environnement depuis le fichier .env pour eviter de mettre les mdp dans le code \n",
        "load_dotenv()\n",
        "\n",
        "# Cr√©er un dictionnaire contenant les informations de connexion √† la base de donn√©es\n",
        "DATABASE_CONFIG = {\n",
        "    'user': os.getenv('DB_USER'), # Nom d'utilisateur pour la base \n",
        "    'password': os.getenv('DB_PASSWORD'),\n",
        "    'host': os.getenv('DB_HOST'),# Adresse du serveur\n",
        "    'port': os.getenv('DB_PORT'),# Port utilis√© pour PostgreSQL\n",
        "    'database': os.getenv('DB_NAME'), \n",
        "}\n",
        "\n",
        "# Cr√©ation de l'engine SQLAlchemy\n",
        "# Cr√©er le moteur SQLAlchemy avec les informations du dictionnaire DATABASE_CONFIG\n",
        "engine = create_engine(\n",
        "    f\"postgresql://{DATABASE_CONFIG['user']}:{DATABASE_CONFIG['password']}@\"\n",
        "     #postgresql : indique le type de base de donn√©es (dialecte), on utilise PostgreSQL.\n",
        "    f\"{DATABASE_CONFIG['host']}:{DATABASE_CONFIG['port']}/{DATABASE_CONFIG['database']}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connexion r√©ussie :\n",
            "                                             version\n",
            "0  PostgreSQL 17.4 on aarch64-unknown-linux-gnu, ...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Test de connexion\n",
        "def test_connection(): # D√©finir une fonction pour tester la connexion √† la base de donn√©es\n",
        "    \"\"\"\n",
        "    Testez votre connexion √† la base\n",
        "\n",
        "    √âtapes :\n",
        "    1. Utilisez pd.read_sql() pour ex√©cuter \"SELECT version()\"\n",
        "    2. Affichez la version PostgreSQL\n",
        "    3. G√©rez les erreurs de connexion\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Ex√©cute une requ√™te SQL pour r√©cup√©rer la version de PostgreSQL\n",
        "        # pd.read_sql() retourne un DataFrame (tableau Pandas)\n",
        "        version = pd.read_sql(\"SELECT version();\", engine)\n",
        "        print(\"Connexion r√©ussie :\")\n",
        "        print(version)\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur de connexion : {e}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "# Ex√©cuter le test\n",
        "test_connection()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXfOgAxGQ3b5"
      },
      "source": [
        "\n",
        "## Partie 2 : Import du dataset e-commerce\n",
        "\n",
        "### üìä Dataset Brazilian E-Commerce\n",
        "Nous utilisons le c√©l√®bre dataset Olist (100k commandes r√©elles).\n",
        "\n",
        "**Tables √† cr√©er :**\n",
        "1. **customers** : customer_id, customer_city, customer_state\n",
        "2. **orders** : order_id, customer_id, order_status, order_date, order_delivered_date\n",
        "3. **order_items** : order_id, product_id, seller_id, price, freight_value\n",
        "4. **products** : product_id, product_category, product_weight_g\n",
        "5. **sellers** : seller_id, seller_city, seller_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1¬∞Chargement des fichiers Olist\n",
        "customers = pd.read_csv(\"data/olist_customers_dataset.csv\")\n",
        "orders = pd.read_csv(\"data/olist_orders_dataset.csv\")\n",
        "order_items = pd.read_csv(\"data/olist_order_items_dataset.csv\")\n",
        "products = pd.read_csv(\"data/olist_products_dataset.csv\")\n",
        "sellers = pd.read_csv(\"data/olist_sellers_dataset.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#2¬∞CREATION TAB\n",
        "def create_tables(engine):\n",
        "    from sqlalchemy import text\n",
        "\n",
        "    create_customers = text(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS customers (\n",
        "        customer_id VARCHAR PRIMARY KEY,\n",
        "        customer_city VARCHAR(100),\n",
        "        customer_state CHAR(2)\n",
        "    );\n",
        "    \"\"\")\n",
        "\n",
        "    create_sellers = text(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS sellers (\n",
        "        seller_id VARCHAR PRIMARY KEY,\n",
        "        seller_city VARCHAR(100),\n",
        "        seller_state CHAR(2)\n",
        "    );\n",
        "    \"\"\")\n",
        "\n",
        "    create_products = text(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS products (\n",
        "        product_id VARCHAR PRIMARY KEY,\n",
        "        product_category VARCHAR,\n",
        "        product_weight_g NUMERIC\n",
        "    );\n",
        "    \"\"\")\n",
        "\n",
        "    create_orders = text(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS orders (\n",
        "        order_id VARCHAR PRIMARY KEY,\n",
        "        customer_id VARCHAR,\n",
        "        order_status VARCHAR(50),\n",
        "        order_date TIMESTAMP,\n",
        "        order_delivered_date TIMESTAMP,\n",
        "        FOREIGN KEY (customer_id) REFERENCES customers(customer_id)\n",
        "    );\n",
        "    \"\"\")\n",
        "\n",
        "    create_order_items = text(\"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS order_items (\n",
        "        order_id VARCHAR,\n",
        "        product_id VARCHAR,\n",
        "        seller_id VARCHAR,\n",
        "        price NUMERIC,\n",
        "        freight_value NUMERIC,\n",
        "        FOREIGN KEY (order_id) REFERENCES orders(order_id),\n",
        "        FOREIGN KEY (product_id) REFERENCES products(product_id),\n",
        "        FOREIGN KEY (seller_id) REFERENCES sellers(seller_id)\n",
        "    );\n",
        "    \"\"\")\n",
        "\n",
        "    with engine.connect() as conn:\n",
        "        conn.execute(create_customers)\n",
        "        conn.execute(create_sellers)\n",
        "        conn.execute(create_products)\n",
        "        conn.execute(create_orders)\n",
        "        conn.execute(create_order_items)\n",
        "        conn.commit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "#3¬∞\n",
        "create_tables(engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Insertion des donn√©es dans Supabase :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Donn√©es ins√©r√©es dans 'customers'\n",
            "Donn√©es ins√©r√©es dans 'sellers'\n",
            "Donn√©es ins√©r√©es dans 'products'\n",
            "Donn√©es ins√©r√©es dans 'orders'\n",
            "Donn√©es ins√©r√©es dans 'order_items' sans doublons.\n"
          ]
        }
      ],
      "source": [
        "#Insertion des donn√©es dans Supabase dans un seule bloc \n",
        "try:\n",
        "    # 1. Insertion des donn√©es dans la table 'customers'\n",
        "    customers_filtered = customers[[\"customer_id\", \"customer_city\", \"customer_state\"]]\n",
        "    customers_filtered.to_sql(\"customers\", engine, if_exists=\"append\", index=False)\n",
        "    print(\"Donn√©es ins√©r√©es dans 'customers'\")\n",
        "\n",
        "    # 2. Insertion des donn√©es dans la table 'sellers'\n",
        "    sellers_filtered = sellers[[\"seller_id\", \"seller_city\", \"seller_state\"]]\n",
        "    sellers_filtered.to_sql(\"sellers\", engine, if_exists=\"append\", index=False)\n",
        "    print(\"Donn√©es ins√©r√©es dans 'sellers'\")\n",
        "\n",
        "    # 3. Insertion des donn√©es dans la table 'products'\n",
        "    products_filtered = products[[\"product_id\", \"product_weight_g\"]]\n",
        "    products_filtered.to_sql(\"products\", engine, if_exists=\"append\", index=False)\n",
        "    print(\"Donn√©es ins√©r√©es dans 'products'\")\n",
        "\n",
        "    # 4. Insertion des donn√©es dans la table 'orders'\n",
        "    orders_renamed = orders.rename(columns={\n",
        "        \"order_purchase_timestamp\": \"order_date\",\n",
        "        \"order_delivered_customer_date\": \"order_delivered_date\"\n",
        "    })\n",
        "    orders_filtered = orders_renamed[[\n",
        "        \"order_id\", \"customer_id\", \"order_status\", \"order_date\", \"order_delivered_date\"\n",
        "    ]]\n",
        "    orders_filtered.to_sql(\"orders\", engine, if_exists=\"append\", index=False)\n",
        "    print(\"Donn√©es ins√©r√©es dans 'orders'\")\n",
        "\n",
        "    # 5. Insertion des donn√©es dans la table 'order_items' (sans doublons sur la cl√© primaire composite)\n",
        "    order_items_filtered = order_items[[\n",
        "        \"order_id\", \"product_id\", \"seller_id\", \"price\", \"freight_value\"\n",
        "    ]].drop_duplicates(subset=[\"order_id\", \"product_id\", \"seller_id\"])\n",
        "    order_items_filtered.to_sql(\"order_items\", engine, if_exists=\"append\", index=False)\n",
        "    print(\"Donn√©es ins√©r√©es dans 'order_items' sans doublons.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(\"Une erreur s‚Äôest produite lors de l‚Äôinsertion des donn√©es :\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3095, 4)\n",
            "(32951, 9)\n",
            "(99441, 8)\n",
            "(112650, 7)\n"
          ]
        }
      ],
      "source": [
        "#9¬∞\n",
        "print(sellers.shape)\n",
        "print(products.shape)\n",
        "print(orders.shape)\n",
        "print(order_items.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBQ_BY-QT4dO"
      },
      "source": [
        "## Partie 3 : Requ√™tes SQL avanc√©es\n",
        "\n",
        "\n",
        "### üîç Analyses SQL √† impl√©menter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdl5RNOBUAV2"
      },
      "source": [
        "#### 1. Analyse RFM (R√©cence, Fr√©quence, Montant)\n",
        "```sql\n",
        "-- Votre d√©fi : Calculer les m√©triques RFM pour chaque client\n",
        "WITH customer_metrics AS (\n",
        "    SELECT\n",
        "        c.customer_id,\n",
        "        c.customer_state,\n",
        "        -- R√©cence : jours depuis dernier achat\n",
        "        -- Fr√©quence : nombre de commandes\n",
        "        -- Montant : total d√©pens√©\n",
        "        \n",
        "        -- Compl√©tez cette requ√™te CTE\n",
        "        \n",
        "    FROM customers c\n",
        "    JOIN orders o ON c.customer_id = o.customer_id\n",
        "    JOIN order_items oi ON o.order_id = oi.order_id\n",
        "    WHERE o.order_status = 'delivered'\n",
        "    GROUP BY c.customer_id, c.customer_state\n",
        ")\n",
        "\n",
        "-- Cr√©ez les segments RFM (Champions, Loyaux, √Ä risque, etc.)\n",
        "SELECT\n",
        "    customer_id,\n",
        "    customer_state,\n",
        "    recency_score,\n",
        "    frequency_score,\n",
        "    monetary_score,\n",
        "    CASE\n",
        "        WHEN recency_score >= 4 AND frequency_score >= 4 THEN 'Champions'\n",
        "        WHEN recency_score >= 3 AND frequency_score >= 3 THEN 'Loyal Customers'\n",
        "        -- Ajoutez les autres segments\n",
        "        ELSE 'Others'\n",
        "    END as customer_segment\n",
        "FROM customer_metrics;\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Requ√™te SQL de base : lire une table\n",
        "from sqlalchemy import text\n",
        "\n",
        "query_customers = \"\"\"\n",
        "SELECT * FROM customers\n",
        "LIMIT 10\n",
        "\"\"\"\n",
        "\n",
        "with engine.begin() as connection:\n",
        "    df_customers = pd.read_sql(query_customers, con=connection)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWF9rpZSUMp5"
      },
      "outputs": [],
      "source": [
        "#### 2. Analyse g√©ographique des ventes\n",
        "\n",
        "def geographic_sales_analysis():\n",
        "    \"\"\"\n",
        "    Analysez les performances par √©tat/r√©gion\n",
        "\n",
        "    Requ√™tes √† √©crire :\n",
        "    1. Top 10 des √©tats par CA\n",
        "    2. Croissance MoM par r√©gion\n",
        "    3. Taux de conversion par ville\n",
        "    4. Distance moyenne vendeur-acheteur\n",
        "    \"\"\"\n",
        "\n",
        "    query_top_states = \"\"\"\n",
        "    -- Votre requ√™te SQL ici\n",
        "    -- Utilisez des JOINs et GROUP BY\n",
        "    -- Calculez le CA, nombre de commandes, panier moyen\n",
        "    \"\"\"\n",
        "\n",
        "    return pd.read_sql(query_top_states, engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OE-UHLKY8-K"
      },
      "source": [
        "#### 3. Analyse temporelle et saisonnalit√©\n",
        "```sql\n",
        "-- D√©tectez les patterns saisonniers\n",
        "SELECT\n",
        "    EXTRACT(YEAR FROM order_date) as year,\n",
        "    EXTRACT(MONTH FROM order_date) as month,\n",
        "    EXTRACT(DOW FROM order_date) as day_of_week,\n",
        "    COUNT(*) as order_count,\n",
        "    SUM(price + freight_value) as total_revenue,\n",
        "    AVG(price + freight_value) as avg_order_value\n",
        "FROM orders o\n",
        "JOIN order_items oi ON o.order_id = oi.order_id\n",
        "WHERE order_status = 'delivered'\n",
        "GROUP BY ROLLUP(\n",
        "    EXTRACT(YEAR FROM order_date),\n",
        "    EXTRACT(MONTH FROM order_date),\n",
        "    EXTRACT(DOW FROM order_date)\n",
        ")\n",
        "ORDER BY year, month, day_of_week;\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq43e3mfZC8d"
      },
      "source": [
        "## Partie 4 : Analyse pr√©dictive avec SQL\n",
        "\n",
        "### üîÆ Mod√®les simples en SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY5mfxFoaL2K"
      },
      "outputs": [],
      "source": [
        "#### 1. Pr√©diction de churn\n",
        "\n",
        "def churn_prediction_sql():\n",
        "    \"\"\"\n",
        "    Identifiez les clients √† risque de churn\n",
        "\n",
        "    Indicateurs :\n",
        "    - Pas d'achat depuis X jours\n",
        "    - Baisse de fr√©quence d'achat\n",
        "    - Diminution du panier moyen\n",
        "    - Changement de comportement g√©ographique\n",
        "    \"\"\"\n",
        "\n",
        "    churn_query = \"\"\"\n",
        "    WITH customer_activity AS (\n",
        "        -- Calculez les m√©triques d'activit√© r√©cente\n",
        "        -- Comparez avec l'historique du client\n",
        "        -- Scorez le risque de churn\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "        customer_id,\n",
        "        days_since_last_order,\n",
        "        order_frequency_trend,\n",
        "        monetary_trend,\n",
        "        churn_risk_score,\n",
        "        CASE\n",
        "            WHEN churn_risk_score > 0.7 THEN 'High Risk'\n",
        "            WHEN churn_risk_score > 0.4 THEN 'Medium Risk'\n",
        "            ELSE 'Low Risk'\n",
        "        END as churn_segment\n",
        "    FROM customer_activity;\n",
        "    \"\"\"\n",
        "\n",
        "    return pd.read_sql(churn_query, engine)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB2D1PDraVu4"
      },
      "source": [
        "#### 2. Recommandations produits\n",
        "```sql\n",
        "-- Market Basket Analysis simplifi√©\n",
        "WITH product_pairs AS (\n",
        "    SELECT\n",
        "        oi1.product_id as product_a,\n",
        "        oi2.product_id as product_b,\n",
        "        COUNT(*) as co_purchase_count\n",
        "    FROM order_items oi1\n",
        "    JOIN order_items oi2 ON oi1.order_id = oi2.order_id\n",
        "    WHERE oi1.product_id != oi2.product_id\n",
        "    GROUP BY oi1.product_id, oi2.product_id\n",
        "    HAVING COUNT(*) >= 10  -- Seuil minimum\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    product_a,\n",
        "    product_b,\n",
        "    co_purchase_count,\n",
        "    co_purchase_count::float / total_a.count as confidence\n",
        "FROM product_pairs pp\n",
        "JOIN (\n",
        "    SELECT product_id, COUNT(*) as count\n",
        "    FROM order_items\n",
        "    GROUP BY product_id\n",
        ") total_a ON pp.product_a = total_a.product_id\n",
        "ORDER BY confidence DESC;\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbYkj8ItabH-"
      },
      "source": [
        "## Partie 5 : Int√©gration avec les APIs m√©t√©o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4CU6SNEfNXb"
      },
      "source": [
        "### üå§Ô∏è Croisement donn√©es m√©t√©o/ventes\n",
        "```python\n",
        "def weather_sales_correlation():\n",
        "    \"\"\"\n",
        "    Correlez vos donn√©es m√©t√©o du Notebook 1 avec les ventes\n",
        "    \n",
        "    Hypoth√®ses √† tester :\n",
        "    1. Les ventes de certaines cat√©gories augmentent-elles avec la pluie ?\n",
        "    2. Y a-t-il un impact de la temp√©rature sur les achats ?\n",
        "    3. Les livraisons sont-elles impact√©es par la m√©t√©o ?\n",
        "    \"\"\"\n",
        "    \n",
        "    # R√©cup√©rez les donn√©es m√©t√©o historiques pour les villes br√©siliennes\n",
        "    weather_query = \"\"\"\n",
        "    SELECT DISTINCT customer_city, customer_state\n",
        "    FROM customers\n",
        "    WHERE customer_state IN ('SP', 'RJ', 'MG', 'RS', 'SC')\n",
        "    ORDER BY customer_city;\n",
        "    \"\"\"\n",
        "    \n",
        "    cities = pd.read_sql(weather_query, engine)\n",
        "    \n",
        "    # Int√©grez avec l'API m√©t√©o\n",
        "    # Analysez les corr√©lations\n",
        "    \n",
        "    pass\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHG9k_5PfZXd"
      },
      "source": [
        "### üìä Dashboard g√©o-temporel\n",
        "```python\n",
        "def create_geotemporal_dashboard():\n",
        "    \"\"\"\n",
        "    Cr√©ez un dashboard interactif combinant :\n",
        "    - Carte des ventes par r√©gion\n",
        "    - √âvolution temporelle avec m√©t√©o\n",
        "    - Segments clients g√©olocalis√©s\n",
        "    - Pr√©dictions par zone g√©ographique\n",
        "    \"\"\"\n",
        "    pass\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsIuD-IVfnxW"
      },
      "source": [
        "---\n",
        "## üèÜ Livrables finaux\n",
        "\n",
        "### üìà Rapport d'analyse complet\n",
        "1. **Segmentation RFM (Recency, Frenquency, Monetary) ** : 5-7 segments avec caract√©ristiques\n",
        "2. **Analyse g√©ographique**  : Performances par r√©gion + recommandations\n",
        "3. **Pr√©dictions churn** : Liste des clients √† risque + actions\n",
        "4. **Recommandations produits** : Top 10 des associations\n",
        "5. **Impact m√©t√©o** : Corr√©lations significatives identifi√©es\n",
        "\n",
        "### üöÄ Pipeline automatis√©\n",
        "```python\n",
        "def automated_analysis_pipeline():\n",
        "    \"\"\"\n",
        "    Pipeline qui :\n",
        "    1. Se connecte √† la DB\n",
        "    2. Ex√©cute toutes les analyses\n",
        "    3. Met √† jour les segments clients\n",
        "    4. G√©n√®re le rapport automatiquement\n",
        "    5. Envoie des alertes si n√©cessaire\n",
        "    \"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wynvmdtNftwf"
      },
      "source": [
        "## üéì Auto-√©valuation\n",
        "\n",
        "- [ ] **Connexion DB** : PostgreSQL fonctionnelle\n",
        "- [ ] **Requ√™tes complexes** : JOINs, CTEs, fonctions analytiques\n",
        "- [ ] **Gestion des erreurs** : Connexions robustes\n",
        "- [ ] **Performance** : Requ√™tes optimis√©es avec index\n",
        "- [ ] **Int√©gration** : SQL + Python + APIs\n",
        "- [ ] **Insights actionables** : Recommandations business claires\n",
        "\n",
        "### üîó Pr√©paration au Notebook 3\n",
        "Le prochain notebook portera sur NoSQL (MongoDB) avec des donn√©es de r√©seaux sociaux et d'IoT, en temps r√©el.\n",
        "\n",
        "### üí° Bases de donn√©es alternatives\n",
        "- **PlanetScale** : MySQL serverless gratuit\n",
        "- **MongoDB Atlas** : 512MB gratuit\n",
        "- **FaunaDB** : Base multi-mod√®le gratuite\n",
        "- **Hasura Cloud** : GraphQL + PostgreSQL"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
